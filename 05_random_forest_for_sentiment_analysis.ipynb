{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest for sentiment analysis\n",
    "\n",
    "In the previous chapter we saw one of the most promising models was based on Random Forest. It combines quite a good accuracy with a performance. It is really important if we'd like to move our model to production, so the best architecture, in terms of its precision, is not always a possible choice. \n",
    "\n",
    "We are going to begin with a simple description of the Random Forest, to understand how it works under the hood. In simple words, Random Forest is a collection of ensembled Decision Trees which vote in order to form a single decision of belonging to a particular class. Each Decision Tree uses a randomly selected subset of features in order to perform its own decision.\n",
    "\n",
    "![Random Forest architecture](images/random-forest.png)\n",
    "\n",
    "## Tuning the model parameters\n",
    "\n",
    "Let's play a little bit with the hyperparameters of the Random Forest Classifier, to optimize its accuracy. First of all, let's list some of the possible parameters and their values: http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "\n",
    "From our perspective the following parameters look like the ones we should test out:\n",
    "\n",
    "- **n_estimators** - number of decision trees used to make a forest, by default set to 10\n",
    "- **criterion** - quality function for measuring a split, can be set to \"gini\" (default) or \"entropy\"\n",
    "- **max_features** - a maximum number of features to consider (int - exact number, float - percentage, \"auto\", \"sqrt\", \"log2\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T14:39:41.813091Z",
     "start_time": "2019-12-03T12:00:35.808778Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.38 s, sys: 1.96 s, total: 6.33 s\n",
      "Wall time: 6.44 s\n",
      "Configuration: n_estimators = 5, criterion = gini, max_features = auto\n",
      "Train accuracy score: 0.9583333333333334\n",
      "Test accuracy score: 0.7062841530054644\n",
      "\n",
      "CPU times: user 3.24 s, sys: 1.32 s, total: 4.56 s\n",
      "Wall time: 4.57 s\n",
      "Configuration: n_estimators = 5, criterion = gini, max_features = log2\n",
      "Train accuracy score: 0.9619193989071039\n",
      "Test accuracy score: 0.6895491803278688\n",
      "\n",
      "CPU times: user 4min 3s, sys: 755 ms, total: 4min 4s\n",
      "Wall time: 4min 4s\n",
      "Configuration: n_estimators = 5, criterion = gini, max_features = None\n",
      "Train accuracy score: 0.957308743169399\n",
      "Test accuracy score: 0.7059426229508197\n",
      "\n",
      "CPU times: user 3.08 s, sys: 740 ms, total: 3.82 s\n",
      "Wall time: 3.82 s\n",
      "Configuration: n_estimators = 5, criterion = entropy, max_features = auto\n",
      "Train accuracy score: 0.9579918032786885\n",
      "Test accuracy score: 0.7015027322404371\n",
      "\n",
      "CPU times: user 1.98 s, sys: 420 ms, total: 2.4 s\n",
      "Wall time: 2.4 s\n",
      "Configuration: n_estimators = 5, criterion = entropy, max_features = log2\n",
      "Train accuracy score: 0.9608948087431693\n",
      "Test accuracy score: 0.694672131147541\n",
      "\n",
      "CPU times: user 1min 5s, sys: 404 ms, total: 1min 5s\n",
      "Wall time: 1min 5s\n",
      "Configuration: n_estimators = 5, criterion = entropy, max_features = None\n",
      "Train accuracy score: 0.9557718579234973\n",
      "Test accuracy score: 0.7185792349726776\n",
      "\n",
      "CPU times: user 6.23 s, sys: 424 ms, total: 6.66 s\n",
      "Wall time: 6.66 s\n",
      "Configuration: n_estimators = 10, criterion = gini, max_features = auto\n",
      "Train accuracy score: 0.9798497267759563\n",
      "Test accuracy score: 0.7349726775956285\n",
      "\n",
      "CPU times: user 3.57 s, sys: 400 ms, total: 3.97 s\n",
      "Wall time: 3.97 s\n",
      "Configuration: n_estimators = 10, criterion = gini, max_features = log2\n",
      "Train accuracy score: 0.983094262295082\n",
      "Test accuracy score: 0.70525956284153\n",
      "\n",
      "CPU times: user 11min 37s, sys: 572 ms, total: 11min 37s\n",
      "Wall time: 11min 37s\n",
      "Configuration: n_estimators = 10, criterion = gini, max_features = None\n",
      "Train accuracy score: 0.9742998633879781\n",
      "Test accuracy score: 0.7257513661202186\n",
      "\n",
      "CPU times: user 23 s, sys: 1.26 s, total: 24.3 s\n",
      "Wall time: 24.3 s\n",
      "Configuration: n_estimators = 10, criterion = entropy, max_features = auto\n",
      "Train accuracy score: 0.9806181693989071\n",
      "Test accuracy score: 0.719603825136612\n",
      "\n",
      "CPU times: user 13.3 s, sys: 1.48 s, total: 14.8 s\n",
      "Wall time: 14.8 s\n",
      "Configuration: n_estimators = 10, criterion = entropy, max_features = log2\n",
      "Train accuracy score: 0.9824965846994536\n",
      "Test accuracy score: 0.7120901639344263\n",
      "\n",
      "CPU times: user 7min 53s, sys: 1.14 s, total: 7min 54s\n",
      "Wall time: 7min 54s\n",
      "Configuration: n_estimators = 10, criterion = entropy, max_features = None\n",
      "Train accuracy score: 0.9771174863387978\n",
      "Test accuracy score: 0.7257513661202186\n",
      "\n",
      "CPU times: user 56.3 s, sys: 1.18 s, total: 57.4 s\n",
      "Wall time: 57.4 s\n",
      "Configuration: n_estimators = 25, criterion = gini, max_features = auto\n",
      "Train accuracy score: 0.9960724043715847\n",
      "Test accuracy score: 0.7404371584699454\n",
      "\n",
      "CPU times: user 31.2 s, sys: 1.23 s, total: 32.4 s\n",
      "Wall time: 32.5 s\n",
      "Configuration: n_estimators = 25, criterion = gini, max_features = log2\n",
      "Train accuracy score: 0.9967554644808743\n",
      "Test accuracy score: 0.7274590163934426\n",
      "\n",
      "CPU times: user 13min 58s, sys: 664 ms, total: 13min 58s\n",
      "Wall time: 13min 59s\n",
      "Configuration: n_estimators = 25, criterion = gini, max_features = None\n",
      "Train accuracy score: 0.9938524590163934\n",
      "Test accuracy score: 0.7332650273224044\n",
      "\n",
      "CPU times: user 15.3 s, sys: 440 ms, total: 15.8 s\n",
      "Wall time: 15.8 s\n",
      "Configuration: n_estimators = 25, criterion = entropy, max_features = auto\n",
      "Train accuracy score: 0.9964993169398907\n",
      "Test accuracy score: 0.7359972677595629\n",
      "\n",
      "CPU times: user 8.75 s, sys: 600 ms, total: 9.35 s\n",
      "Wall time: 9.35 s\n",
      "Configuration: n_estimators = 25, criterion = entropy, max_features = log2\n",
      "Train accuracy score: 0.9967554644808743\n",
      "Test accuracy score: 0.73224043715847\n",
      "\n",
      "CPU times: user 6min 12s, sys: 460 ms, total: 6min 12s\n",
      "Wall time: 6min 12s\n",
      "Configuration: n_estimators = 25, criterion = entropy, max_features = None\n",
      "Train accuracy score: 0.9948770491803278\n",
      "Test accuracy score: 0.73224043715847\n",
      "\n",
      "CPU times: user 38.3 s, sys: 584 ms, total: 38.8 s\n",
      "Wall time: 38.8 s\n",
      "Configuration: n_estimators = 50, criterion = gini, max_features = auto\n",
      "Train accuracy score: 0.9973531420765027\n",
      "Test accuracy score: 0.7404371584699454\n",
      "\n",
      "CPU times: user 21.4 s, sys: 428 ms, total: 21.8 s\n",
      "Wall time: 21.8 s\n",
      "Configuration: n_estimators = 50, criterion = gini, max_features = log2\n",
      "Train accuracy score: 0.9976092896174863\n",
      "Test accuracy score: 0.73224043715847\n",
      "\n",
      "CPU times: user 19min 6s, sys: 612 ms, total: 19min 7s\n",
      "Wall time: 19min 7s\n",
      "Configuration: n_estimators = 50, criterion = gini, max_features = None\n",
      "Train accuracy score: 0.9968408469945356\n",
      "Test accuracy score: 0.7383879781420765\n",
      "\n",
      "CPU times: user 57.3 s, sys: 644 ms, total: 57.9 s\n",
      "Wall time: 57.9 s\n",
      "Configuration: n_estimators = 50, criterion = entropy, max_features = auto\n",
      "Train accuracy score: 0.9973531420765027\n",
      "Test accuracy score: 0.7421448087431693\n",
      "\n",
      "CPU times: user 21.7 s, sys: 488 ms, total: 22.2 s\n",
      "Wall time: 22.2 s\n",
      "Configuration: n_estimators = 50, criterion = entropy, max_features = log2\n",
      "Train accuracy score: 0.9976092896174863\n",
      "Test accuracy score: 0.7318989071038251\n",
      "\n",
      "CPU times: user 14min 49s, sys: 756 ms, total: 14min 50s\n",
      "Wall time: 14min 51s\n",
      "Configuration: n_estimators = 50, criterion = entropy, max_features = None\n",
      "Train accuracy score: 0.997011612021858\n",
      "Test accuracy score: 0.7346311475409836\n",
      "\n",
      "CPU times: user 1min 20s, sys: 488 ms, total: 1min 21s\n",
      "Wall time: 1min 21s\n",
      "Configuration: n_estimators = 100, criterion = gini, max_features = auto\n",
      "Train accuracy score: 0.9976946721311475\n",
      "Test accuracy score: 0.7431693989071039\n",
      "\n",
      "CPU times: user 41.7 s, sys: 576 ms, total: 42.3 s\n",
      "Wall time: 42.3 s\n",
      "Configuration: n_estimators = 100, criterion = gini, max_features = log2\n",
      "Train accuracy score: 0.9976946721311475\n",
      "Test accuracy score: 0.7332650273224044\n",
      "\n",
      "CPU times: user 36min 5s, sys: 688 ms, total: 36min 6s\n",
      "Wall time: 36min 6s\n",
      "Configuration: n_estimators = 100, criterion = gini, max_features = None\n",
      "Train accuracy score: 0.9976092896174863\n",
      "Test accuracy score: 0.735655737704918\n",
      "\n",
      "CPU times: user 1min 40s, sys: 448 ms, total: 1min 40s\n",
      "Wall time: 1min 40s\n",
      "Configuration: n_estimators = 100, criterion = entropy, max_features = auto\n",
      "Train accuracy score: 0.9976946721311475\n",
      "Test accuracy score: 0.744535519125683\n",
      "\n",
      "CPU times: user 39.1 s, sys: 532 ms, total: 39.6 s\n",
      "Wall time: 39.6 s\n",
      "Configuration: n_estimators = 100, criterion = entropy, max_features = log2\n",
      "Train accuracy score: 0.9976946721311475\n",
      "Test accuracy score: 0.7353142076502732\n",
      "\n",
      "CPU times: user 32min 16s, sys: 1.3 s, total: 32min 17s\n",
      "Wall time: 32min 32s\n",
      "Configuration: n_estimators = 100, criterion = entropy, max_features = None\n",
      "Train accuracy score: 0.9976946721311475\n",
      "Test accuracy score: 0.7370218579234973\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%run 02_data_preparation.ipynb\n",
    "\n",
    "import itertools\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "N_ESTIMATORS = (5, 10, 25, 50, 100)\n",
    "CRITERION = (\"gini\", \"entropy\")\n",
    "MAX_FEATURES = (\"auto\", \"log2\", None)\n",
    "\n",
    "# Divide the dataset into train and test fraction\n",
    "train_messages, test_messages, train_targets, test_targets = train_test_split(tweets[\"text\"], \n",
    "                                                                              tweets[\"sentiment\"],\n",
    "                                                                              test_size=0.2)\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "for n_estimators, criterion, max_features in itertools.product(N_ESTIMATORS,\n",
    "                                                               CRITERION,\n",
    "                                                               MAX_FEATURES):\n",
    "    # Define the classifier instance\n",
    "    classifier = RandomForestClassifier(random_state=2018, \n",
    "                                        n_estimators=n_estimators, \n",
    "                                        criterion=criterion, \n",
    "                                        max_features=max_features)\n",
    "    # Vectorize preprocessed sentences\n",
    "    train_features = vectorizer.fit_transform(train_messages)\n",
    "\n",
    "    # Train the model\n",
    "    %time fit = classifier.fit(train_features.toarray(), train_targets)\n",
    "\n",
    "    # Check the accuracy of the model on test data and display it\n",
    "    test_features = vectorizer.transform(test_messages)\n",
    "    train_predictions = fit.predict(train_features.toarray())\n",
    "    train_accuracy = accuracy_score(train_predictions, train_targets)\n",
    "    test_predictions = fit.predict(test_features.toarray())\n",
    "    test_accuracy = accuracy_score(test_predictions, test_targets)\n",
    "    print(\"Configuration: n_estimators = {}, criterion = {}, max_features = {}\\n\"\n",
    "          \"Train accuracy score: {}\\n\"\n",
    "          \"Test accuracy score: {}\\n\".format(n_estimators, criterion, max_features, \n",
    "                                             train_accuracy, test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turned out the following configuration achieves the best accuracy on our test dataset:\n",
    "\n",
    "`n_estimators = 100, criterion = entropy, max_features = auto`\n",
    "\n",
    "For that reason we are going to create a simple application that will use these parameters for training. That will be a console application reading the sentences from the user and classifies its sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercise\n",
    "\n",
    "Please fill the gaps in the source code in *exercise/exercise_03.py*, in order to create an application that will:\n",
    "\n",
    "- create Random Forest based model and train it on a whole dataset used in previous examples, using selected parameters (`n_estimators = 100, criterion = entropy, max_features = auto`)\n",
    "- continiously read the sentences from the standard input and classify them with the created model, until \"exit\" sentence is passed\n",
    "- display the probabilities of beloning to each class\n",
    "\n",
    "Some of the functionalities are already prepared - please complete the source code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/solution/exercise_03.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  tweets[\"text\"] = tweets[\"text\"].map(preprocess_text)\n",
      "/home/jovyan/solution/exercise_03.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  tweets[\"sentiment\"] = tweets[\"sentiment\"].map(lambda x: SENTIMENT_TO_LABEL_MAPPING[x])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence ('exit' to close): That was a horrible experience\n",
      "Sentence: That was a horrible experience\n",
      "Probabilities: {-1: 0.5, 0: 0.39000000000000001, 1: 0.11}\n",
      "Sentence ('exit' to close): Thanks United, I enjoyed the flight :)\n",
      "Sentence: Thanks United, I enjoyed the flight :)\n",
      "Probabilities: {-1: 0.23999999999999999, 0: 0.28000000000000003, 1: 0.47999999999999998}\n",
      "Sentence ('exit' to close): exit\n"
     ]
    }
   ],
   "source": [
    "%run exercise/exercise_03.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importance\n",
    "\n",
    "One of the biggest advantages of Random Forest classifier is the ability to describe the importance of the used features. It allows to check which variables have the best predictive force and to understand how the model performs the decision. The following code snippet visualizes the feature importance for our created model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T15:37:01.763855Z",
     "start_time": "2019-12-03T15:37:01.227171Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>thanks</th>\n",
       "      <td>0.054820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>thank</th>\n",
       "      <td>0.049368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jetblue</th>\n",
       "      <td>0.031004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>southwestair</th>\n",
       "      <td>0.028089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>0.018536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>0.016540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>united</th>\n",
       "      <td>0.015479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>you</th>\n",
       "      <td>0.014598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>virginamerica</th>\n",
       "      <td>0.014444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>americanair</th>\n",
       "      <td>0.014245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no</th>\n",
       "      <td>0.012814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>great</th>\n",
       "      <td>0.012522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>0.012448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>not</th>\n",
       "      <td>0.011342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>on</th>\n",
       "      <td>0.011205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>flight</th>\n",
       "      <td>0.011035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>usairways</th>\n",
       "      <td>0.010949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>for</th>\n",
       "      <td>0.010492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>co</th>\n",
       "      <td>0.010359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is</th>\n",
       "      <td>0.010181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hold</th>\n",
       "      <td>0.010076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>your</th>\n",
       "      <td>0.010064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>my</th>\n",
       "      <td>0.010021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>can</th>\n",
       "      <td>0.008959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>in</th>\n",
       "      <td>0.008029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http</th>\n",
       "      <td>0.007971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>0.007918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>it</th>\n",
       "      <td>0.007492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>but</th>\n",
       "      <td>0.007171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hours</th>\n",
       "      <td>0.006756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>please</th>\n",
       "      <td>0.002623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>up</th>\n",
       "      <td>0.002573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>help</th>\n",
       "      <td>0.002481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>an</th>\n",
       "      <td>0.002407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>flights</th>\n",
       "      <td>0.002381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fly</th>\n",
       "      <td>0.002231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>they</th>\n",
       "      <td>0.002137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>need</th>\n",
       "      <td>0.002098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>plane</th>\n",
       "      <td>0.002088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kudos</th>\n",
       "      <td>0.002030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all</th>\n",
       "      <td>0.002022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yes</th>\n",
       "      <td>0.002012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>would</th>\n",
       "      <td>0.001978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bag</th>\n",
       "      <td>0.001928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>us</th>\n",
       "      <td>0.001908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amp</th>\n",
       "      <td>0.001825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>how</th>\n",
       "      <td>0.001825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>know</th>\n",
       "      <td>0.001818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>about</th>\n",
       "      <td>0.001733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>crew</th>\n",
       "      <td>0.001724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>well</th>\n",
       "      <td>0.001719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>our</th>\n",
       "      <td>0.001715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>am</th>\n",
       "      <td>0.001709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>flying</th>\n",
       "      <td>0.001708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>when</th>\n",
       "      <td>0.001704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gate</th>\n",
       "      <td>0.001700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>or</th>\n",
       "      <td>0.001679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>like</th>\n",
       "      <td>0.001657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>as</th>\n",
       "      <td>0.001618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worst</th>\n",
       "      <td>0.001592</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               importance\n",
       "thanks           0.054820\n",
       "thank            0.049368\n",
       "jetblue          0.031004\n",
       "southwestair     0.028089\n",
       "to               0.018536\n",
       "the              0.016540\n",
       "united           0.015479\n",
       "you              0.014598\n",
       "virginamerica    0.014444\n",
       "americanair      0.014245\n",
       "no               0.012814\n",
       "great            0.012522\n",
       "and              0.012448\n",
       "not              0.011342\n",
       "on               0.011205\n",
       "flight           0.011035\n",
       "usairways        0.010949\n",
       "for              0.010492\n",
       "co               0.010359\n",
       "is               0.010181\n",
       "hold             0.010076\n",
       "your             0.010064\n",
       "my               0.010021\n",
       "can              0.008959\n",
       "in               0.008029\n",
       "http             0.007971\n",
       "love             0.007918\n",
       "it               0.007492\n",
       "but              0.007171\n",
       "hours            0.006756\n",
       "...                   ...\n",
       "please           0.002623\n",
       "up               0.002573\n",
       "help             0.002481\n",
       "an               0.002407\n",
       "flights          0.002381\n",
       "fly              0.002231\n",
       "they             0.002137\n",
       "need             0.002098\n",
       "plane            0.002088\n",
       "kudos            0.002030\n",
       "all              0.002022\n",
       "yes              0.002012\n",
       "would            0.001978\n",
       "bag              0.001928\n",
       "us               0.001908\n",
       "amp              0.001825\n",
       "how              0.001825\n",
       "know             0.001818\n",
       "about            0.001733\n",
       "crew             0.001724\n",
       "well             0.001719\n",
       "our              0.001715\n",
       "am               0.001709\n",
       "flying           0.001708\n",
       "when             0.001704\n",
       "gate             0.001700\n",
       "or               0.001679\n",
       "like             0.001657\n",
       "as               0.001618\n",
       "worst            0.001592\n",
       "\n",
       "[100 rows x 1 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "feature_importances = pd.DataFrame(classifier.feature_importances_, \n",
    "                                   index=vectorizer.get_feature_names(),\n",
    "                                   columns=(\"importance\", ))\n",
    "feature_importances = feature_importances.sort_values(\"importance\", \n",
    "                                                      ascending=False)\n",
    "feature_importances.head(n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T15:37:22.131614Z",
     "start_time": "2019-12-03T15:37:22.076689Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ggreenwald</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>getyourlife</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>getyouracttogether</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gettingthirsty</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gettingimpatient</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gettin</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>getmorehands</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>getmeouttahere</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>getmeoffrhisfuckinplane</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zzps5ywve2</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         importance\n",
       "ggreenwald                      0.0\n",
       "getyourlife                     0.0\n",
       "getyouracttogether              0.0\n",
       "gettingthirsty                  0.0\n",
       "gettingimpatient                0.0\n",
       "gettin                          0.0\n",
       "getmorehands                    0.0\n",
       "getmeouttahere                  0.0\n",
       "getmeoffrhisfuckinplane         0.0\n",
       "zzps5ywve2                      0.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importances.tail(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
